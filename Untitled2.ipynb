{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "words=\"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "# Get the words in each line\n",
    "    words = line.split()\n",
    "# Generate the count for each word\n",
    "for word in words:\n",
    "# Write the key-value pair to stdout to be processed by\n",
    "# the reducer.\n",
    "# The key is anything before the first tab character and the #value is anything after the first tab character.\n",
    "    print ('{0}\\t{1}'.format(word, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fb7b3b65a273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# If the current word is the same as the previous word, # increment its count, otherwise print the words count # to stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcurr_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcurr_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "curr_word = None\n",
    "curr_count = 0\n",
    "# Process each key-value pair from the mapper\n",
    "for line in sys.stdin:\n",
    "# Get the key and value from the current line\n",
    "    word, count = line.split('\\t') # Convert the count to an int\n",
    "    count = int(count)\n",
    "# If the current word is the same as the previous word, # increment its count, otherwise print the words count # to stdout\n",
    "if word == curr_word:\n",
    "    curr_count += count \n",
    "else:\n",
    "# Write word and its number of occurrences as a key-value # pair to stdout\n",
    "    if curr_word:\n",
    "        print ('{0}\\t{1}'.format(curr_word, curr_count))\n",
    "        curr_word = word\n",
    "# Output the count for the last word\n",
    "    if curr_word == word:\n",
    "        print ('{0}\\t{1}'.format(curr_word, curr_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-ea60a8ead17f>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-ea60a8ead17f>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    matches = normalized_title.filter(lambda x: reques ted_movie.value in x[0])\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import re\n",
    "import sys\n",
    "def main():\n",
    "   # Insure a search term was supplied at the command line\n",
    "    if len(sys.argv) != 2:\n",
    "        sys.stderr.write('Usage: {} <search_term>'.format(sys.argv[0]))\n",
    "        sys.exit()\n",
    "   # Create the SparkContext\n",
    "    sc = SparkContext(appName='SparkWordCount') # Broadcast the requested term\n",
    "    requested_movie = sc.broadcast(sys.argv[1]) # Load the input file\n",
    "    source_file = sc.textFile('/user/hduser/input/movies') # Get the movie title from the second fields\n",
    "    titles = source_file.map(lambda line: line.split('|')[1])\n",
    "       # Create a map of the normalized title to the raw title\n",
    "    normalized_title = titles.map(lambda title: (re.sub(r'\\s*\\ (\\d{4}\\)','', title).lower(), title))\n",
    "       # Find all movies matching the requested_movie\n",
    "    matches = normalized_title.filter(lambda x: reques ted_movie.value in x[0])\n",
    "   # Collect all the matching titles\n",
    "    matching_titles = matches.map(lambda x: x[1]).distinct().col lect()\n",
    "       # Display the result\n",
    "    print ('{} Matching titles found:'.format(len(matching_titles)))\n",
    "    for title in matching_titles:\n",
    "    \n",
    "        print (title) \n",
    "        sc.stop()\n",
    "if __name__ == '__main__': \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: /Applications/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py <search_term>"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "\n",
    "   # Insure a search term was supplied at the command line\n",
    "   if len(sys.argv) != 2:\n",
    "      sys.stderr.write('Usage: {} <search_term>'.format(sys.argv[0]))\n",
    "      sys.exit()\n",
    "\n",
    "   # Create the SparkContext\n",
    "   sc = SparkContext(appName='SparkWordCount')\n",
    "\n",
    "   # Broadcast the requested term\n",
    "   requested_movie = sc.broadcast(sys.argv[1])\n",
    "\n",
    "   # Load the input file\n",
    "   source_file = sc.textFile('/user/hduser/input/movies')\n",
    "\n",
    "   # Get the movie title from the second fields\n",
    "   titles = source_file.map(lambda line: line.split('|')[1])\n",
    "\n",
    "   # Create a map of the normalized title to the raw title\n",
    "   normalized_title = titles.map(lambda title: (re.sub(r'\\s*\\(\\d{4}\\)','', title).lower(), title))\n",
    "   \n",
    "   # Find all movies matching the requested_movie\n",
    "   matches = normalized_title.filter(lambda x: requested_movie.value in x[0])\n",
    "\n",
    "   # Collect all the matching titles\n",
    "   matching_titles = matches.map(lambda x: x[1]).distinct().collect()\n",
    "\n",
    "   # Display the result\n",
    "   print ('{} Matching titles found:'.format(len(matching_titles)))\n",
    "   for title in matching_titles:\n",
    "      print (title)\n",
    "\n",
    "   sc.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'appName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d03f20ef4f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'appName' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"terminal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"terminal.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/Users/recepkabatas/Library/Jupyter/runtime/kernel-40c23cc3-e474-49e1-ac0e-8ed6727b0d5b.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f6dc1736617f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# get threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# read in text file and split each document into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/Users/recepkabatas/Library/Jupyter/runtime/kernel-40c23cc3-e474-49e1-ac0e-8ed6727b0d5b.json'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  # create Spark context with Spark configuration\n",
    "  conf = SparkConf().setAppName(\"Spark Count\")\n",
    "  \n",
    "\n",
    "  # get threshold\n",
    "  threshold = int(sys.argv[2])\n",
    "\n",
    "  # read in text file and split each document into words\n",
    "  tokenized = sc.textFile(sys.argv[1]).flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "  # count the occurrence of each word\n",
    "  wordCounts = tokenized.map(lambda word: (word, 1)).reduceByKey(lambda v1,v2:v1 +v2)\n",
    "\n",
    "  # filter out words with fewer than threshold occurrences\n",
    "  filtered = wordCounts.filter(lambda pair:pair[1] >= threshold)\n",
    "\n",
    "  # count characters\n",
    "  charCounts = filtered.flatMap(lambda pair:pair[0]).map(lambda c: c).map(lambda c: (c, 1)).reduceByKey(lambda v1,v2:v1 +v2)\n",
    "\n",
    "  list = charCounts.collect()\n",
    "  print (repr(list)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
