{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [the] Frequency: 9\n",
      "Word: [life] Frequency: 1\n",
      "Word: [situations] Frequency: 1\n",
      "Word: [since] Frequency: 1\n",
      "Word: [day] Frequency: 4\n",
      "Word: [hdfs] Frequency: 11\n",
      "Word: [hadoop] Frequency: 8\n"
     ]
    }
   ],
   "source": [
    "#kelime ayiklama\n",
    "file = open('terminal.txt', 'r')\n",
    "book = file.read()\n",
    "\n",
    "\n",
    "def tokenize():\n",
    "    if book is not None:\n",
    "        words = book.lower().split()\n",
    "        return words\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def map_book(tokens):\n",
    "    hash_map = {}\n",
    "\n",
    "    if tokens is not None:\n",
    "        for element in tokens:\n",
    "            # Remove Punctuation\n",
    "            word = element.replace(\",\",\"\")\n",
    "            word = word.replace(\".\",\"\")\n",
    "\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Tokenize the Book\n",
    "words = tokenize()\n",
    "word_list = ['the','life','situations','since','day','hdfs','hadoop',]\n",
    "\n",
    "# Create a Hash Map (Dictionary)\n",
    "map = map_book(words)\n",
    "\n",
    "# Show Word Information\n",
    "for word in word_list:\n",
    "    print ('Word: [' + word + '] Frequency: ' + str(map[word]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelled\n",
      "\n",
      "<function <lambda> at 0x107b52e18>\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from time import sleep\n",
    "result = \"Not Set\"\n",
    "lock = threading.Lock()\n",
    "def map_func(x):\n",
    "    sleep(100)\n",
    "    raise Exception(\"Task should have been cancelled\")\n",
    "def start_job(x):\n",
    "    global result\n",
    "    try:\n",
    "        sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
    "        result = sc.parallelize(range(x)).map(map_func).collect()\n",
    "    except Exception as e:\n",
    "        result = \"Cancelled\"\n",
    "    lock.release()\n",
    "def stop_job():\n",
    "    sleep(5)\n",
    "    sc.cancelJobGroup(\"job_to_cancel\")\n",
    "supress = lock.acquire()\n",
    "supress = threading.Thread(target=start_job, args=(10,)).start()\n",
    "supress = threading.Thread(target=stop_job).start()\n",
    "supress = lock.acquire()\n",
    "print(result)\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print()\n",
    "print(combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -mkdir /usr/cloudera',\n",
       " \"mkdir: `/usr/cloudera': No such file or directory\",\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -mkdir /user/cloudera/WordCount',\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -mkdir /user/cloudera/WordCount/input',\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/Desktop/input.txt user/cloudera/WordCount/input',\n",
       " \"put: `user/cloudera/WordCouhdfs nt/input': No such file or directory\",\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/Desktop/input.txt /user/cloudera/WordCount/input',\n",
       " '[cloudera@quickstart ~]$ hadoop dfs -jar /home/cloudera/D',\n",
       " 'Desktop/   Documents/ Downloads/ ',\n",
       " '[cloudera@quickstart ~]$ ',\n",
       " '[cloudera@quickstart ~]$ hadoop dfs -jar /home/cloudera/Desktop/wordcount.jar wordcount wordcoun^C/user/cloudera/WordCount/input /user/cloudera/WordCount/output ',\n",
       " '[cloudera@quickstart ~]$ ^C',\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/Desktop/input.txt /user/cloudera/WordCount/input',\n",
       " \"put: `/user/cloudera/WordCount/input/input.txt': File exists\",\n",
       " '[cloudera@quickstart ~]$ hadoop dfs -jar /home/cloudera/Desktop/Untitled.jar wordcount.wordcount /user/cloudera/Desktop/input /user/cloudera/Desktop/output',\n",
       " 'DEPRECATED: Use of this script to execute hdfs command is deprecated.',\n",
       " 'Instead use the hdfs command for it.',\n",
       " '',\n",
       " '-jar: Unknown command',\n",
       " '[cloudera@quickstart ~]$ hadoop dfs jar /home/cloudera/Desktop/Untitled.jar wordcount.wordcount /user/cloudera/Desktop/input /user/cloudera/Desktop/output',\n",
       " 'DEPRECATED: Use of this script to execute hdfs command is deprecated.',\n",
       " 'Instead use the hdfs command for it.',\n",
       " '',\n",
       " 'jar: Unknown command',\n",
       " '[cloudera@quickstart ~]$ hadoop jar /home/cloudera/Desktop/Untitled.jar wordcount.wordcount /user/cloudera/Desktop/input /user/cloudera/Desktop/output',\n",
       " '16/08/24 04:15:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032',\n",
       " '16/08/24 04:15:06 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.',\n",
       " '16/08/24 04:15:06 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/cloudera/.staging/job_1471515428094_0001',\n",
       " '16/08/24 04:15:06 WARN security.UserGroupInformation: PriviledgedActionException as:cloudera (auth:SIMPLE) cause:org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://quickstart.cloudera:8020/user/cloudera/Desktop/input',\n",
       " 'Exception in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://quickstart.cloudera:8020/user/cloudera/Desktop/input',\n",
       " '\\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)',\n",
       " '\\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)',\n",
       " '\\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)',\n",
       " '\\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:305)',\n",
       " '\\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:322)',\n",
       " '\\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)',\n",
       " '\\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)',\n",
       " '\\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)',\n",
       " '\\tat java.security.AccessController.doPrivileged(Native Method)',\n",
       " '\\tat javax.security.auth.Subject.doAs(Subject.java:415)',\n",
       " '\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)',\n",
       " '\\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)',\n",
       " '\\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325)',\n",
       " '\\tat wordcount.wordcount.main(wordcount.java:62)',\n",
       " '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)',\n",
       " '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)',\n",
       " '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)',\n",
       " '\\tat java.lang.reflect.Method.invoke(Method.java:606)',\n",
       " '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)',\n",
       " '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)',\n",
       " '[cloudera@quickstart ~]$ hadoop jar /home/cloudera/Desktop/Untitled.jar wordcount.wordcount /user/cloudera/WordCount/input  /user/cloudera/WordCount/output',\n",
       " '16/08/24 04:16:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032',\n",
       " '16/08/24 04:16:34 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.',\n",
       " '16/08/24 04:16:35 INFO input.FileInputFormat: Total input paths to process : 1',\n",
       " '16/08/24 04:16:35 INFO mapreduce.JobSubmitter: number of splits:1',\n",
       " '16/08/24 04:16:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1471515428094_0002',\n",
       " '16/08/24 04:16:36 INFO impl.YarnClientImpl: Submitted application application_1471515428094_0002',\n",
       " '16/08/24 04:16:36 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1471515428094_0002/',\n",
       " '16/08/24 04:16:36 INFO mapreduce.Job: Running job: job_1471515428094_0002',\n",
       " '16/08/24 04:16:47 INFO mapreduce.Job: Job job_1471515428094_0002 running in uber mode : false',\n",
       " '16/08/24 04:16:47 INFO mapreduce.Job:  map 0% reduce 0%',\n",
       " '16/08/24 04:16:54 INFO mapreduce.Job:  map 100% reduce 0%',\n",
       " '16/08/24 04:17:03 INFO mapreduce.Job:  map 100% reduce 100%',\n",
       " '16/08/24 04:17:03 INFO mapreduce.Job: Job job_1471515428094_0002 completed successfully',\n",
       " '16/08/24 04:17:03 INFO mapreduce.Job: Counters: 49',\n",
       " '\\tFile System Counters',\n",
       " '\\t\\tFILE: Number of bytes read=330',\n",
       " '\\t\\tFILE: Number of bytes written=227863',\n",
       " '\\t\\tFILE: Number of read operations=0',\n",
       " '\\t\\tFILE: Number of large read operations=0',\n",
       " '\\t\\tFILE: Number of write operations=0',\n",
       " '\\t\\tHDFS: Number of bytes read=313',\n",
       " '\\t\\tHDFS: Number of bytes written=220',\n",
       " '\\t\\tHDFS: Number of read operations=6',\n",
       " '\\t\\tHDFS: Number of large read operations=0',\n",
       " '\\t\\tHDFS: Number of write operations=2',\n",
       " '\\tJob Counters ',\n",
       " '\\t\\tLaunched map tasks=1',\n",
       " '\\t\\tLaunched reduce tasks=1',\n",
       " '\\t\\tData-local map tasks=1',\n",
       " '\\t\\tTotal time spent by all maps in occupied slots (ms)=4946',\n",
       " '\\t\\tTotal time spent by all reduces in occupied slots (ms)=6071',\n",
       " '\\t\\tTotal time spent by all map tasks (ms)=4946',\n",
       " '\\t\\tTotal time spent by all reduce tasks (ms)=6071',\n",
       " '\\t\\tTotal vcore-seconds taken by all map tasks=4946',\n",
       " '\\t\\tTotal vcore-seconds taken by all reduce tasks=6071',\n",
       " '\\t\\tTotal megabyte-seconds taken by all map tasks=5064704',\n",
       " '\\t\\tTotal megabyte-seconds taken by all reduce tasks=6216704',\n",
       " '\\tMap-Reduce Framework',\n",
       " '\\t\\tMap input records=1',\n",
       " '\\t\\tMap output records=27',\n",
       " '\\t\\tMap output bytes=285',\n",
       " '\\t\\tMap output materialized bytes=330',\n",
       " '\\t\\tInput split bytes=136',\n",
       " '\\t\\tCombine input records=27',\n",
       " '\\t\\tCombine output records=26',\n",
       " '\\t\\tReduce input groups=26',\n",
       " '\\t\\tReduce shuffle bytes=330',\n",
       " '\\t\\tReduce input records=26',\n",
       " '\\t\\tReduce output records=26',\n",
       " '\\t\\tSpilled Records=52',\n",
       " '\\t\\tShuffled Maps =1',\n",
       " '\\t\\tFailed Shuffles=0',\n",
       " '\\t\\tMerged Map outputs=1',\n",
       " '\\t\\tGC time elapsed (ms)=125',\n",
       " '\\t\\tCPU time spent (ms)=1350',\n",
       " '\\t\\tPhysical memory (bytes) snapshot=332062720',\n",
       " '\\t\\tVirtual memory (bytes) snapshot=3007524864',\n",
       " '\\t\\tTotal committed heap usage (bytes)=226562048',\n",
       " '\\tShuffle Errors',\n",
       " '\\t\\tBAD_ID=0',\n",
       " '\\t\\tCONNECTION=0',\n",
       " '\\t\\tIO_ERROR=0',\n",
       " '\\t\\tWRONG_LENGTH=0',\n",
       " '\\t\\tWRONG_MAP=0',\n",
       " '\\t\\tWRONG_REDUCE=0',\n",
       " '\\tFile Input Format Counters ',\n",
       " '\\t\\tBytes Read=177',\n",
       " '\\tFile Output Format Counters ',\n",
       " '\\t\\tBytes Written=220',\n",
       " '[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/WordCount/output/*',\n",
       " '(including\\t1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re, string\n",
    "\n",
    "text_file = sc.textFile('terminal.txt')\n",
    "text_file.take(123)#n adet satiri getirir.indexli olarak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
